{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%ls -lah"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLbXwcZaPFNl",
        "outputId": "4da01bd4-2d67-4215-b6ee-05e2f5d4c8a9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 20M\n",
            "drwxr-xr-x 1 root root 4.0K May 12 20:08 \u001b[0m\u001b[01;34m.\u001b[0m/\n",
            "drwxr-xr-x 1 root root 4.0K May 12 16:48 \u001b[01;34m..\u001b[0m/\n",
            "-rw-r--r-- 1 root root 3.2M May 12 16:56 19600101.txt\n",
            "drwxr-xr-x 4 root root 4.0K May 11 16:34 \u001b[01;34m.config\u001b[0m/\n",
            "drwxr-xr-x 2 root root 4.0K May 12 20:08 \u001b[01;34m.ipynb_checkpoints\u001b[0m/\n",
            "drwxr-xr-x 1 root root 4.0K May 11 16:35 \u001b[01;34msample_data\u001b[0m/\n",
            "-rw-r--r-- 1 root root  17M May 12 18:37 temp.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head temp.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ml5A7t0nlFEM",
        "outputId": "febc8d3a-aeac-45c7-ec1c-5644a52f4fe2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datetime,datenum,data,year,month,lat\n",
            "15-Jan-0000 12:00:00,15.5,277.519897460938,0,1,-82.5\n",
            "15-Jan-0000 12:00:00,15.5,277.413146972656,0,1,-67.5\n",
            "15-Jan-0000 12:00:00,15.5,277.559020996094,0,1,-52.5\n",
            "15-Jan-0000 12:00:00,15.5,277.692993164062,0,1,-37.5\n",
            "15-Jan-0000 12:00:00,15.5,277.851440429688,0,1,-22.5\n",
            "15-Jan-0000 12:00:00,15.5,277.736114501953,0,1,-7.5\n",
            "15-Jan-0000 12:00:00,15.5,277.867553710938,0,1,7.5\n",
            "15-Jan-0000 12:00:00,15.5,278.610809326172,0,1,22.5\n",
            "15-Jan-0000 12:00:00,15.5,279.152557373047,0,1,37.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEXK9ei0XS36",
        "outputId": "2251d02d-3518-45a9-9667-b00764bbf240"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.4.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lon = []\n",
        "for x in range(-180, 182, 2):\n",
        "  lon.append([float(x)])"
      ],
      "metadata": {
        "id": "KwpQHfK5YGFp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import to_timestamp\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import Row\n",
        "from datetime import datetime\n",
        "import reverse_geocoder as rg \n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.functions import countDistinct\n",
        "\n",
        "def reverseGeocode(c1, c2):\n",
        "    result = rg.search([c1, c2])\n",
        "    return result[0]['cc']"
      ],
      "metadata": {
        "id": "pAFodU_QjCF4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc = SparkContext.getOrCreate()\n",
        "rdd = sc.textFile('co2_csv_data.csv')\n",
        "header = rdd.first()\n",
        "rdd = rdd.filter(lambda line: line != header)\n",
        "\n",
        "longit = []\n",
        "for x in range(-180, 180, 2):\n",
        "  longit.append(x)\n",
        "\n",
        "# Extract the desired columns\n",
        "rdd_final = rdd.map(lambda line: line.split(','))\\\n",
        "  .map(lambda row: (int(row[3]), int(row[4]), float(row[2]), float(row[5])))\\\n",
        "  .filter(lambda row: row[0] >= 1960)\\\n",
        "  .flatMap(lambda row: [(row[0],row[1],row[2],row[3], x) for x in longit])\\\n",
        "  .map(lambda row: ( (row[0], row[1],reverseGeocode(row[3], row[4])),  row[2]))\\\n",
        "  .reduceByKey(lambda x, y: x)\n",
        "\n",
        "for row in rdd_final.take(10):\n",
        "    print(row)"
      ],
      "metadata": {
        "id": "NjXvuaCi3IGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_rdd = rdd_final.map(lambda row: ','.join(map(str, row)))\n",
        "save_rdd.saveAsTextFile('big_data_co2_country_monthly.csv')"
      ],
      "metadata": {
        "id": "YLp9YwXge1lz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}